title: Unsocial VR
date: 2017-07-14 10:00
order: 02
img: unsocial-vr.png
summary: Fake social behaviours in shared virtual environment
links: source code;https://github.com/Nagasaki45/UnsocialVR;fab fa-github

Advance Project Placement, Media and Arts Technology doctoral programme,
Queen Mary University of London. Supervised by Prof. Patrick Healey and
hosted by [Inition](https://www.inition.co.uk/).

<div class="youtube youtube-16x9">
<iframe src="https://www.youtube.com/embed/tqbtOL5R4fw" allowfullscreen seamless frameBorder="0"></iframe>
</div>

## What?

In Infinite Jest, David Foster Wallace argues that *"Good old
traditional audio-only phone conversations allowed you to presume that
the person on the other end was paying complete attention to you while
also permitting you not to have to pay anything even close to complete
attention to her."* He continues and claims that we are addicted to
this illusion, and that's why video conferencing always feel so
awkward - we need to pretend to listen all the time. And if we think
about it, even in face to face conversation we must always adhere to
these social rules, and signal our complete attention when someone is
talking to us.

In this project I experiment with VR technologies to see if this
illusion of faking active listening is transferable to other mediums,
and if so, how. In Unsocial VR participants share the same virtual
environment, using the HTC Vive headset and controllers. They can
converse freely and move around, and if you want to start faking
listening to the conversation and just wander around, or even talk with
other participants while faking, you absolutely can! The interface is
very minimal, just hit a button on the controller to start faking active
listening behaviours towards your current conversation, and release it
when you want to stop faking. You will even get an on screen
notification when someone is speaking directly to you, so you can return
to the conversation elegantly.

This project is based on multidisciplinary research. It merges ideas
from telepresence and mediated communication, that explore ways to
represent non verbal cues in new communication technologies. It depends
on multiparty social interaction analysis, which take spatial cues into
account in understanding who is talking with whom in a "cocktail
party" situation. It explores new ways to generate active listening
signals (or "backchannel behaviours", as they called in the scientific
literature), such as head nods, in automated agents.

Further information can be found in my
[mid-project](https://youtu.be/K39_wlQ60-Y) and
[inter/sections](https://youtu.be/2k8MO74guTA) presentations.
